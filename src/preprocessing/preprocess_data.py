"""
Loads the processed data from `data/processed_heart_disease.csv`,
separates features (X) and target (y), defines categorical and numerical features,
applies transformations (StandardScaler for numerical, OneHotEncoder for categorical)
using a ColumnTransformer, splits the data into training and testing sets,
and saves these sets along with the fitted ColumnTransformer (preprocessor) object.
"""
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
import os
import numpy as np
import joblib # Added for saving the preprocessor

PROCESSED_DATA_PATH = "data/processed_heart_disease.csv"
PREPROCESSOR_PATH = "data/preprocessor.joblib" # Path to save preprocessor
TRAIN_X_PATH = "data/train_X.csv"
TRAIN_Y_PATH = "data/train_y.csv"
TEST_X_PATH = "data/test_X.csv"
TEST_Y_PATH = "data/test_y.csv"

def preprocess_data():
    """Loads processed data, performs preprocessing, and saves train/test sets."""
    if not os.path.exists(PROCESSED_DATA_PATH):
        print(f"Error: Processed data not found at {PROCESSED_DATA_PATH}. Please run load_data.py first.")
        return

    print(f"Loading data from {PROCESSED_DATA_PATH}...")
    df = pd.read_csv(PROCESSED_DATA_PATH)

    print("Separating features (X) and target (y)...")
    X = df.drop("target", axis=1)
    y = df["target"]

    # Define categorical and numerical features
    # Note: This is a common division, but specific features might be treated differently
    # based on deeper domain knowledge or EDA.
    # For example, 'sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal' are often treated as categorical.
    # 'ca' is numerical but has a limited range and could be treated as categorical.

    categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal', 'ca']
    # Ensure all categorical features are present in X's columns
    categorical_features = [col for col in categorical_features if col in X.columns]
    print(f"Identified categorical features: {categorical_features}")

    numerical_features = [col for col in X.columns if col not in categorical_features]
    print(f"Identified numerical features: {numerical_features}")

    # Create preprocessing pipelines for numerical and categorical features
    # Numerical features will be scaled.
    # Categorical features will be one-hot encoded. 'handle_unknown='ignore'' means if a new category
    # appears during transform (e.g., in API input), it will be encoded as all zeros for those OHE columns.
    numerical_pipeline = StandardScaler()
    categorical_pipeline = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

    # Create a ColumnTransformer to apply the defined transformations
    print("Defining ColumnTransformer for preprocessing...")
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_pipeline, numerical_features),
            ('cat', categorical_pipeline, categorical_features)
        ],
        remainder='passthrough' # Any columns not specified in transformers are passed through unchanged.
                                # If all columns are specified, this has no effect.
    )

    print("Fitting ColumnTransformer and transforming features X...")
    # Fit the preprocessor on the entire feature set X and transform it.
    # In a strict pipeline, you'd fit only on X_train to prevent data leakage from X_test into the fit of the preprocessor.
    # However, for simplicity in this project structure where preprocess_data runs before train/test split of X_processed,
    # fitting on all X is done here. The API will use this preprocessor fitted on all X.
    X_processed = preprocessor.fit_transform(X)

    # Get feature names after one-hot encoding
    # Access the OneHotEncoder from the preprocessor
    ohe_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)
    all_feature_names = numerical_features + list(ohe_feature_names)

    # Convert X_processed (numpy array) back to a DataFrame with proper column names
    # These names include the original numerical feature names and the new names generated by OneHotEncoder.
    X_processed_df = pd.DataFrame(X_processed, columns=all_feature_names, index=X.index)
    print(f"X_processed_df created with shape: {X_processed_df.shape}")

    # Split the processed data into training and testing sets
    # stratify=y ensures that the proportion of the target variable is similar in both train and test sets.
    print("Splitting data into training and testing sets (80/20)...")
    X_train, X_test, y_train, y_test = train_test_split(X_processed_df, y, test_size=0.2, random_state=42, stratify=y)

    # Save preprocessed data splits
    print(f"Saving training and testing data to {os.path.dirname(TRAIN_X_PATH)}...")
    X_train.to_csv(TRAIN_X_PATH, index=False)
    y_train.to_csv(TRAIN_Y_PATH, index=False, header=True) # save y_train with header
    X_test.to_csv(TEST_X_PATH, index=False)
    y_test.to_csv(TEST_Y_PATH, index=False, header=True) # save y_test with header

    # Save the preprocessor
    if not os.path.exists("data"):
        os.makedirs("data")
    joblib.dump(preprocessor, PREPROCESSOR_PATH)
    print(f"Preprocessor saved to {PREPROCESSOR_PATH}")

    print(f"Preprocessed training and testing data saved to data/ directory.")
    print(f"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}")
    print(f"Number of features after preprocessing: {X_train.shape[1]}")
    print(f"Final shapes: X_train: {X_train.shape}, y_train: {y_train.shape}, X_test: {X_test.shape}, y_test: {y_test.shape}")

if __name__ == "__main__":
    print("Starting data preprocessing...")
    preprocess_data()
    print("Data preprocessing finished.")
